import sys
import datetime
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.utils import getResolvedOptions
from pyspark.sql.functions import (
    col, to_date, count, lit, concat_ws, year, dayofweek, round
)
from pyspark.sql import functions as F

# ---------------------------------------------
# Glue initialization
# ---------------------------------------------
args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# ---------------------------------------------
# Constants
# ---------------------------------------------
START_DATE = "2024-01-01"
END_DATE = "2024-12-31"
YEAR = 2024

# ---------------------------------------------
# Paths
# ---------------------------------------------
leave_data_path = "s3://ttn-de-bootcamp-2025-bronze-us-east-1/Tashu/gud3/silver/emp_leave_data/"
leave_quota_path = "s3://ttn-de-bootcamp-2025-bronze-us-east-1/Tashu/gud3/silver/leave_quota/"
holiday_path = "s3://ttn-de-bootcamp-2025-bronze-us-east-1/Tashu/gud3/silver/leave_cal/"
alert_output_path = "s3://ttn-de-bootcamp-2025-bronze-us-east-1/Tashu/gud3/gold/leave_alert_emails/"
alert_tracking_path = alert_output_path + "alerted_employees.parquet"

# ---------------------------------------------
# Load leave data
# ---------------------------------------------
leave_data_df = spark.read.parquet(leave_data_path).withColumn("date", to_date(col("date")))

# ---------------------------------------------
# Load holiday data
# ---------------------------------------------
holiday_df = spark.read.parquet(holiday_path).withColumn("date", to_date(col("date")))

# ---------------------------------------------
# Filter ACTIVE leaves in date range
# ---------------------------------------------
availed_leaves_df = leave_data_df.filter(
    (col("status") == "ACTIVE") &
    (col("date") >= F.lit(START_DATE)) &
    (col("date") <= F.lit(END_DATE)) &
    (year("date") == YEAR)
).select("emp_id", "date").dropDuplicates()

# Exclude holidays
availed_leaves_df = availed_leaves_df.join(
    holiday_df.select("date"),
    on="date",
    how="left_anti"
)

# Exclude weekends
availed_leaves_df = availed_leaves_df.withColumn("dow", dayofweek("date")) \
    .filter(~col("dow").isin([1, 7])) \
    .drop("dow")

# ---------------------------------------------
# Count availed leaves per emp
# ---------------------------------------------
leaves_taken_df = availed_leaves_df.groupBy("emp_id").agg(count("date").alias("availed_leaves"))

# ---------------------------------------------
# Load leave quota for 2024
# ---------------------------------------------
leave_quota_df = spark.read.parquet(leave_quota_path).withColumn("year", col("year").cast("int"))
leave_quota_2024_df = leave_quota_df.filter(col("year") == YEAR)

# ---------------------------------------------
# Join and calculate usage %
# ---------------------------------------------
leave_usage_df = leaves_taken_df.join(
    leave_quota_2024_df.select("emp_id", "leave_quota"),
    on="emp_id",
    how="inner"
)

leave_usage_df = leave_usage_df.withColumn(
    "used_percent", (col("availed_leaves") / col("leave_quota")) * 100
)

# ---------------------------------------------
# Filter high usage (>80%)
# ---------------------------------------------
high_usage_df = leave_usage_df.filter(col("used_percent") > 80)

# ---------------------------------------------
# Avoid duplicate alerts
# ---------------------------------------------
try:
    alerted_emps_df = spark.read.parquet(alert_tracking_path)
except:
    alerted_emps_df = spark.createDataFrame([], high_usage_df.schema)

new_alerts_df = high_usage_df.join(
    alerted_emps_df.select("emp_id"),
    on="emp_id",
    how="left_anti"
)

# ---------------------------------------------
# Simulate email alert content
# ---------------------------------------------
email_content_df = new_alerts_df.withColumn(
    "message",
    concat_ws(
        " ",
        lit("Dear Manager,"),
        lit("Employee with ID"),
        col("emp_id"),
        lit("has used"),
        round(col("used_percent"), 2),
        lit("% of their leave quota in 2024.")
    )
).select("emp_id", "message")

# ---------------------------------------------
# Save alerts
# ---------------------------------------------
if email_content_df.count() > 0:
    email_content_df.select(col("message").alias("value")) \
        .write.mode("append") \
        .text(alert_output_path + f"emails/year=2024")

    new_alerts_df.select("emp_id", "availed_leaves", "leave_quota", "used_percent") \
        .write.mode("append").parquet(alert_tracking_path)

# ---------------------------------------------
# Logging
# ---------------------------------------------
print("✅ Total qualifying leaves:", availed_leaves_df.count())
print("✅ High usage (>80%) employees:", high_usage_df.count())
print("✅ New alerts generated:", email_content_df.count())
email_content_df.show(truncate=False)

job.commit()
